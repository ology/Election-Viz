#!/usr/bin/perl

=head1 PROGRAM

Convert the wikipedia C<wikitable> for presidential election results, by state,
into a columnar data file.

=head2 AUTHOR

Gene Boggs C<gene plus github at ology dot net>

=head2 TO DO

Investigate L<MediaWiki::DumpFile>

=cut

use strict;
use warnings;

use HTML::TableExtract;
use LWP::UserAgent;

# Get the year from the commandline.
my $year = shift || 2012;

# Cached raw data file.
my $filename = sprintf 'data/html-%d.txt', $year;

# String to hold the harvest or read results.
my $html = '';

# Cache or harvest?
if (-e $filename) {
    warn "Cached data exists in $filename\n";
    # Read the cashed wikipedia text.
    local $/;
    open my $input, '<', $filename or die "Can't read $filename: $!";
    # Set the text for matching.
    $html = <$input>;
}
else {
    # Harvest from wikipedia.
    my $url = 'https://en.wikipedia.org/wiki/';
    $url .= 'United_States_presidential_election,_' . $year;
    warn "Harvesting new data from entry: '$url'\n";
    my $agent = LWP::UserAgent->new;
    my $response = $agent->get($url);

    # Cache the raw entry.
    $html = $response->content;
    open my $output, '>', $filename or die "Can't write to $filename: $!";
    print $output $html;
}

# Define a TableExtract object and parse the html entry with it.
my $tex = HTML::TableExtract->new(depth => 0, count => 10);
$tex->parse($html);
# 
for my $tab ($tex->tables) {
    for my $row ($tab->rows) {
        print join(',', map { defined $_ ? $_ : '' } @$row), "\n";
    }
}
